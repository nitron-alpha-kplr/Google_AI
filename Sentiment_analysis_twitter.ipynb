{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nitron-alpha-kplr/Google_AI/blob/main/Sentiment_analysis_twitter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.Depencencies"
      ],
      "metadata": {
        "id": "LN5U2aaAdxma"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTIFQAXdwrL4",
        "outputId": "30431016-4c4b-4101-fcaf-a27a65b28820"
      },
      "source": [
        "pip install nltk==3.3"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nltk==3.3\n",
            "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk==3.3) (1.16.0)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394470 sha256=31acaf4924da2dc856401207d890c48c99c77760b5414cc0190d54a34f87a086\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/6d/14/3defa4cd7013faeddf715150696f4a96d7725c87700eb8a68e\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.8.1\n",
            "    Uninstalling nltk-3.8.1:\n",
            "      Successfully uninstalled nltk-3.8.1\n",
            "Successfully installed nltk-3.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99SWc0q2w3w0",
        "outputId": "44340e06-053e-4af9-b414-51f51cd0b431"
      },
      "source": [
        "import nltk\n",
        "nltk.download('twitter_samples') # Le corpus Twitter de NLTK contient actuellement un échantillon de 20k Tweets (nommés ' twitter_samples ') récupérés à partir de l'API Twitter Streaming.\n",
        "nltk.download('punkt') # Punkt Sentence Tokenizer. Ce tokenizer divise un texte en une liste de phrases en utilisant un algorithme non supervisé pour construire un modèle pour les mots d'abréviation, les collocations et les mots qui commencent les phrases. Il doit être entraîné sur une grande collection de textes en clair dans la langue cible avant de pouvoir être utilisé.\n",
        "nltk.download('wordnet') # WordNet est une base de données lexicale pour la langue anglaise, qui a été créée par Princeton, et fait partie du corpus NLTK. Vous pouvez utiliser WordNet avec le module NLTK pour trouver le sens des mots, les synonymes, les antonymes, et plus encore.\n",
        "nltk.download('averaged_perceptron_tagger') #The averaged_perceptron_tagger.zip contains the pre-trained English https://en.wikipedia.org/wiki/Part_of_speech\n",
        "nltk.download('stopwords') #mots vides"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls /root/nltk_data/corpora"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQW7kAibe2SQ",
        "outputId": "176631e4-f2eb-4476-cbfd-168feb9c71b0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mstopwords\u001b[0m/  stopwords.zip  \u001b[01;34mtwitter_samples\u001b[0m/  twitter_samples.zip  wordnet.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"wordnet\" doit être décompressé manuellement"
      ],
      "metadata": {
        "id": "YsCM9x5Zd5aF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "file_loc = '/root/nltk_data/corpora/wordnet.zip'\n",
        "with ZipFile(file_loc, 'r') as z:\n",
        "  z.extractall('/root/nltk_data/corpora/')"
      ],
      "metadata": {
        "id": "mrd5-4hQeo7H"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import twitter_samples, stopwords\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import FreqDist, classify, NaiveBayesClassifier\n",
        "\n",
        "import re, string, random"
      ],
      "metadata": {
        "id": "PHMg0CKEZSlG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.Définition des fonctions de traitement de texte"
      ],
      "metadata": {
        "id": "uUgiVHtxfJQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_noise(tweet_tokens, stop_words = ()):\n",
        "\n",
        "    cleaned_tokens = []\n",
        "\n",
        "    for token, tag in pos_tag(tweet_tokens):\n",
        "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
        "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
        "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
        "\n",
        "        if tag.startswith(\"NN\"):\n",
        "            pos = 'n'\n",
        "        elif tag.startswith('VB'):\n",
        "            pos = 'v'\n",
        "        else:\n",
        "            pos = 'a'\n",
        "\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        token = lemmatizer.lemmatize(token, pos)\n",
        "\n",
        "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
        "            cleaned_tokens.append(token.lower())\n",
        "    return cleaned_tokens"
      ],
      "metadata": {
        "id": "3cyrvJ9IZW-l"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_words(cleaned_tokens_list):\n",
        "    for tokens in cleaned_tokens_list:\n",
        "        for token in tokens:\n",
        "            yield token"
      ],
      "metadata": {
        "id": "o-WvmF1IZbL_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tweets_for_model(cleaned_tokens_list):\n",
        "    for tweet_tokens in cleaned_tokens_list:\n",
        "        yield dict([token, True] for token in tweet_tokens)"
      ],
      "metadata": {
        "id": "W0nLNnmAZdy-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.Construire le dataset à partir du corpus propre"
      ],
      "metadata": {
        "id": "yC2jOHjyfcXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "text = twitter_samples.strings('tweets.20150430-223406.json')\n",
        "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')[0]"
      ],
      "metadata": {
        "id": "aMbR7ugybOY7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english')"
      ],
      "metadata": {
        "id": "6D3khgpnbSrh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
        "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')"
      ],
      "metadata": {
        "id": "HiT1K4QTbXwG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_cleaned_tokens_list = []\n",
        "negative_cleaned_tokens_list = []\n",
        "\n",
        "for tokens in positive_tweet_tokens:\n",
        "    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
        "\n",
        "for tokens in negative_tweet_tokens:\n",
        "    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))"
      ],
      "metadata": {
        "id": "rXZgw1pXbeLV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_pos_words = get_all_words(positive_cleaned_tokens_list)\n",
        "\n",
        "freq_dist_pos = FreqDist(all_pos_words)\n",
        "print(freq_dist_pos.most_common(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHMzPRpBbnBc",
        "outputId": "aa10678e-a0c2-4042-8db6-7a496c188e54"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
        "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\n",
        "\n",
        "positive_dataset = [(tweet_dict, \"Positive\")\n",
        "                        for tweet_dict in positive_tokens_for_model]\n",
        "\n",
        "negative_dataset = [(tweet_dict, \"Negative\")\n",
        "                        for tweet_dict in negative_tokens_for_model]\n",
        "\n",
        "dataset = positive_dataset + negative_dataset\n",
        "\n",
        "random.shuffle(dataset) #mélanger arbitrairement\n",
        "\n"
      ],
      "metadata": {
        "id": "hmO6lfBwbwfX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.Le modèle Naive Bayes classifier"
      ],
      "metadata": {
        "id": "7ceirnNefzwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = dataset[:7000]\n",
        "test_data = dataset[7000:]"
      ],
      "metadata": {
        "id": "YLms-IJEgFL5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = NaiveBayesClassifier.train(train_data)\n",
        "\n",
        "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
        "print(classifier.show_most_informative_features(20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Fy9_Mf_b0hf",
        "outputId": "cc2f97fa-e4cc-47f0-e3d4-16d2dd1496de"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy is: 0.9963333333333333\n",
            "Most Informative Features\n",
            "                      :( = True           Negati : Positi =   2044.3 : 1.0\n",
            "                      :) = True           Positi : Negati =   1678.0 : 1.0\n",
            "                follower = True           Positi : Negati =     38.5 : 1.0\n",
            "                 welcome = True           Positi : Negati =     33.1 : 1.0\n",
            "                  arrive = True           Positi : Negati =     31.0 : 1.0\n",
            "                     sad = True           Negati : Positi =     30.7 : 1.0\n",
            "                followed = True           Negati : Positi =     23.1 : 1.0\n",
            "                     x15 = True           Negati : Positi =     16.6 : 1.0\n",
            "                     bam = True           Positi : Negati =     15.3 : 1.0\n",
            "               community = True           Positi : Negati =     15.3 : 1.0\n",
            "               goodnight = True           Positi : Negati =     14.0 : 1.0\n",
            "                    tire = True           Negati : Positi =     12.7 : 1.0\n",
            "           unfortunately = True           Negati : Positi =     11.4 : 1.0\n",
            "                     via = True           Positi : Negati =     11.2 : 1.0\n",
            "              appreciate = True           Positi : Negati =     11.2 : 1.0\n",
            "                    glad = True           Positi : Negati =     11.2 : 1.0\n",
            "                    miss = True           Negati : Positi =     10.5 : 1.0\n",
            "                   thank = True           Positi : Negati =     10.4 : 1.0\n",
            "                 anymore = True           Negati : Positi =     10.1 : 1.0\n",
            "                   great = True           Positi : Negati =      8.9 : 1.0\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.Utiliser le modèle pour classer son tweet"
      ],
      "metadata": {
        "id": "Ktr0OKkBgOn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_tweet = \"I ordered just once from TerribleCo, they screwed up, never used the app again.\"\n",
        "\n",
        "custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
        "\n",
        "print(custom_tweet, classifier.classify(dict([token, True] for token in custom_tokens)))"
      ],
      "metadata": {
        "id": "jwzF9tiALWVX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c717e20-0c93-4a18-d7e7-13369e4d1f1a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I ordered just once from TerribleCo, they screwed up, never used the app again. Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "C'est à vous maintenant de tester et challenger l'algorithme!"
      ],
      "metadata": {
        "id": "o7cjV_jmh256"
      }
    }
  ]
}